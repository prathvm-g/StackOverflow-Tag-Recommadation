{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation and Ranking Analysis\n",
        "\n",
        "## Objective\n",
        "Evaluate the performance of the multi-label Stack Overflow tag recommendation model using ranking-based evaluation metrics.  \n",
        "This notebook measures how effectively the trained Linear SVM model recommends relevant tags for each question.\n",
        "\n",
        "## Evaluation Metrics\n",
        "The following metrics are used to assess model performance:\n",
        "\n",
        "- **Micro F1 Score**  \n",
        "  Measures overall multi-label classification performance across all tags.\n",
        "\n",
        "- **Precision@5**  \n",
        "  Evaluates how many of the top-5 recommended tags are actually correct.  \n",
        "  This metric reflects real-world tag recommendation quality.\n",
        "\n",
        "- **Hamming Loss**  \n",
        "  Measures the fraction of incorrect labels to the total number of labels.\n",
        "\n",
        "## Steps Performed\n",
        "1. Loaded trained model, encoder, and vectorizer artifacts\n",
        "2. Generated prediction scores using the Linear SVM decision function\n",
        "3. Selected Top-5 tags for each sample using ranking-based selection\n",
        "4. Computed evaluation metrics including Micro F1, Precision@5, and Hamming Loss\n",
        "5. Analyzed prediction quality to understand ranking performance\n",
        "\n",
        "## Outcome\n",
        "This evaluation provides insight into the model’s ability to recommend relevant tags and highlights areas for further ranking and feature improvements.\n"
      ],
      "metadata": {
        "id": "iuusUZ3f6-Qq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Model Artifacts\n",
        "\n",
        "The trained classifier, TF-IDF vectorizer, and label encoder are loaded to reproduce the prediction pipeline exactly as used during training.  \n",
        "This ensures evaluation results reflect the actual deployed model behavior.\n"
      ],
      "metadata": {
        "id": "jIYgc_fnYmUG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oub00l9dX8SH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "df= pd.read_parquet(\"modeling.parquet\")\n",
        "\n",
        "mlb = joblib.load(\"mlb_encoder.pkl\")\n",
        "model = joblib.load(\"tag_prediction_model.pkl\")\n",
        "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Evaluation Data\n",
        "\n",
        "The cleaned question text is transformed into TF-IDF feature vectors using the saved vectorizer.  \n",
        "Ground-truth labels are encoded into multi-label format to enable proper multi-label evaluation.\n"
      ],
      "metadata": {
        "id": "Mom9n2c4ZJi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = vectorizer.transform(df[\"clean_text\"])\n",
        "Y = mlb.transform(df[\"filtered_tags\"])"
      ],
      "metadata": {
        "id": "nKBzVZisYSlS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Train test split**"
      ],
      "metadata": {
        "id": "f8smGOIQZbuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X,Y, test_size=0.2 , random_state= 42\n",
        ")"
      ],
      "metadata": {
        "id": "IuMEq9l8ZYoG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Tag Predictions\n",
        "\n",
        "The trained Linear SVM model produces decision scores for each tag.  \n",
        "Top-K ranking based on these scores is used to simulate real-world tag recommendation behavior, where the system suggests the most relevant tags rather than making binary predictions.\n"
      ],
      "metadata": {
        "id": "KSzd7U12ZyrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "print(Y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i4DsYTgZx4t",
        "outputId": "397001f6-ca4e-4254-c8f5-ddaf00d28d46"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calcuating top 5 tags"
      ],
      "metadata": {
        "id": "jJR_JrKb7gnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.decision_function(X_test)"
      ],
      "metadata": {
        "id": "AT6pMz1Q4NXd"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "top_k = 5\n",
        "Y_pred_topk = np.zeros_like(scores)\n",
        "\n",
        "for i in range(len(scores)):\n",
        "    # rank positions (higher rank → higher weight)\n",
        "    order = np.argsort(scores[i])\n",
        "    rank_weight = np.zeros(scores.shape[1])\n",
        "    rank_weight[order] = np.linspace(0, 1, len(order))\n",
        "\n",
        "    # combine margin + rank signal\n",
        "    combined_score = scores[i] + 0.25 * rank_weight\n",
        "\n",
        "    top_indices = np.argsort(combined_score)[-top_k:]\n",
        "    Y_pred_topk[i, top_indices] = 1\n"
      ],
      "metadata": {
        "id": "O_10Vz521xyW"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "Model performance is evaluated using ranking-aware multi-label metrics:\n",
        "\n",
        "- **Micro F1 Score:** Measures overall prediction balance across labels\n",
        "- **Precision@5:** Evaluates correctness of the top recommended tags\n",
        "- **Hamming Loss:** Measures the fraction of incorrectly predicted labels\n",
        "\n",
        "These metrics provide a more realistic assessment for recommendation systems.\n"
      ],
      "metadata": {
        "id": "TfGaNr83aJ-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Micro F1 Score"
      ],
      "metadata": {
        "id": "6G-Dk7MvaqnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "micro_f1 = f1_score(Y_test, Y_pred, average=\"micro\")\n",
        "\n",
        "print(\"Micro F1:\", micro_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3AJG4U6Z8Ra",
        "outputId": "ac6e3e53-ff31-439c-c542-96dc16dd6959"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro F1: 0.6886113279555902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Precision@5"
      ],
      "metadata": {
        "id": "WmGrARFra9j2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "for i in range(len(scores)):\n",
        "    pred_tags = set(np.where(Y_pred_topk[i] == 1)[0])\n",
        "    true_tags = set(np.where(Y_test[i] == 1)[0])\n",
        "    correct += len(pred_tags & true_tags)\n",
        "\n",
        "precision_at_5 = correct / (len(scores) * top_k)\n",
        "print(\"Precision@5:\", precision_at_5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvrkNHg3a6RW",
        "outputId": "39c2e782-6647-46cd-b865-45f5b4568b56"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision@5: 0.25417561075334766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hamming loss"
      ],
      "metadata": {
        "id": "rp39OtUeatHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import hamming_loss\n",
        "\n",
        "print(\"Hamming loss:\", hamming_loss(Y_test, Y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPu1dfF4aYWH",
        "outputId": "1853bcad-cab3-4019-dc42-0ce8d967c6c3"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hamming loss: 0.014678523970152305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results Interpretation\n",
        "\n",
        "The trained multi-label tag recommendation model achieved a **Micro-F1 score of ~0.68**, indicating that the classifier captures meaningful relationships between question text and associated tags across the selected tag space.\n",
        "\n",
        "The **Precision@5 score of ~0.25** reflects the inherent difficulty of multi-label tag recommendation, where multiple relevant tags compete within a limited Top-5 recommendation window. Despite this challenge, the model consistently identifies at least one or more relevant tags among its top suggestions.\n",
        "\n",
        "The relatively low **Hamming Loss (~0.014)** suggests that the overall label-wise prediction error remains small, confirming that the model maintains stable multi-label prediction behavior.\n",
        "\n",
        "These results demonstrate that feature engineering and ranking strategy play a significant role in recommendation performance, providing a strong baseline for further improvements through enhanced text representations and ranking calibration.\n"
      ],
      "metadata": {
        "id": "ZaR7L0V59XQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction Analysis\n",
        "\n",
        "Example predictions are examined to understand the qualitative performance of the model.  \n",
        "This step helps identify common error patterns, tag confusion, and areas where feature representation may limit prediction accuracy.\n"
      ],
      "metadata": {
        "id": "_-Ql_qzTdZBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = 10\n",
        "\n",
        "print(\"Text:\\n\", df.iloc[sample_idx][\"clean_text\"][:500])\n",
        "print(\"\\nActual tags:\", df.iloc[sample_idx][\"filtered_tags\"])\n",
        "\n",
        "pred_tags = mlb.inverse_transform(\n",
        "    model.predict(X[sample_idx])\n",
        ")\n",
        "\n",
        "print(\"Predicted tags: \", pred_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl7ULOgQde65",
        "outputId": "e5daae1c-3ed3-4ddd-bd95-d04c5b9c573e"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:\n",
            " ios 6 apps - how to deal with iphone 5 screen size? \n",
            "possible duplicate:\n",
            "how to develop or migrate apps for iphone 5 screen resolution? \n",
            "\n",
            "i was just wondering with how should we deal with the iphone 5 bigger screen size.\n",
            "as it has more pixels in height, things like gcrectmake that use coordinates (and just doubled the pixels with the retina/non retina problem) won't work seamlessly between versions, as it happened when we got the retina.\n",
            "and will we have to design two storyboards, just like for \n",
            "\n",
            "Actual tags: ['iphone' 'ios']\n",
            "Predicted tags:  [('ios', 'iphone')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a18f988"
      },
      "source": [
        "More Prediction Random Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5003ed1b",
        "outputId": "73a11cbe-1802-4594-d885-ef4be4d8e652"
      },
      "source": [
        "import random\n",
        "\n",
        "# Generate a few random sample indices to show diverse examples\n",
        "sample_indices = random.sample(range(len(df)), 5)\n",
        "\n",
        "for sample_idx in sample_indices:\n",
        "    print(f\"\\n--- Sample Index: {sample_idx} ---\")\n",
        "    print(\"Text:\\n\", df.iloc[sample_idx][\"clean_text\"][:500])\n",
        "    print(\"\\nActual tags:\", df.iloc[sample_idx][\"filtered_tags\"])\n",
        "\n",
        "    # Reshape X[sample_idx] to be 2D for prediction (model expects 2D input)\n",
        "    pred_tags = mlb.inverse_transform(model.predict(X[sample_idx].reshape(1, -1)))\n",
        "\n",
        "    print(\"Predicted tags: \", pred_tags)\n",
        "    print(\"-----------------------------------\")"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample Index: 41064 ---\n",
            "Text:\n",
            " cakephp: using models in different controllers i have a controller/model for projects. so this controls the projects model, etc, etc. i have a homepage which is being controlled by the pages_controller. i want to show a list of projects on the homepage. is it as easy as doing:\n",
            "function index() {\n",
            "    $this->set('projects', $this->project->find('all'));        \n",
            "}\n",
            "\n",
            "i'm guessing not as i'm getting:\n",
            "undefined property: pagescontroller::$project\n",
            "\n",
            "can someone steer me in the right direction please,\n",
            "jon\n",
            "\n",
            "Actual tags: ['php']\n",
            "Predicted tags:  [('php',)]\n",
            "-----------------------------------\n",
            "\n",
            "--- Sample Index: 38159 ---\n",
            "Text:\n",
            " why illegal start of declaration in scala? for the following code:\n",
            "package fileoperations\n",
            "import java.net.url\n",
            "\n",
            "object fileoperations {\n",
            "    def processwindowspath(p: string): string {\n",
            "        \"file:///\" + p.replaceall(\"\\\\\", \"/\")\n",
            "    }\n",
            "}\n",
            "\n",
            "compiler gives an error:\n",
            "> scalac fileoperations.scala\n",
            "fileoperations.scala:6: error: illegal start of declaration\n",
            "        \"file:///\" + p.replaceall(\"\\\\\", \"/\")\n",
            "\n",
            "why? how to fix?\n",
            "\n",
            "Actual tags: ['scala']\n",
            "Predicted tags:  [('scala',)]\n",
            "-----------------------------------\n",
            "\n",
            "--- Sample Index: 31537 ---\n",
            "Text:\n",
            " observablecollection loses binding when i \"new\" it i have a listbox on my ui that is bound to a property of observablecollection.  i set a new instance of the observablecollection into the property in the view model's constructor and i can add items to it with a button on the form.  these are visible in the list.\n",
            "all is good.\n",
            "however, if i reinitialize the property with new in the button callback, it breaks the binding and the ui no longer shows what is in the collection.\n",
            "i assumed the binding w\n",
            "\n",
            "Actual tags: ['c#' 'wpf']\n",
            "Predicted tags:  [('c#', 'wpf')]\n",
            "-----------------------------------\n",
            "\n",
            "--- Sample Index: 30863 ---\n",
            "Text:\n",
            " learning and using augmented bayes classifiers in python i'm trying to use a forest (or tree) augmented bayes classifier (original introduction, learning) in python (preferably python 3, but python 2 would also be acceptable), first learning it (both structure and parameter learning) and then using it for discrete classification and obtaining probabilities for those features with missing data. (this is why just discrete classification and even good naive classifiers are not very useful for me.)\n",
            "\n",
            "\n",
            "Actual tags: ['python']\n",
            "Predicted tags:  [('python',)]\n",
            "-----------------------------------\n",
            "\n",
            "--- Sample Index: 16332 ---\n",
            "Text:\n",
            " rails find getting activerecord::recordnotfound i have a table that includes a \"belongs to\" in the model.\n",
            "the table includes the xx_id field to link the two tables.\n",
            "but, sometimes the xx_id is going to be blank.\n",
            "when it is, i get activerecord::recordnotfound.\n",
            "i don't want an error - i just want a blank display for this field.\n",
            "what do you suggest?\n",
            "\n",
            "Actual tags: ['ruby-on-rails']\n",
            "Predicted tags:  [('ruby-on-rails',)]\n",
            "-----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and Key Insights\n",
        "\n",
        "This project developed a multi-label NLP system to recommend relevant tags for Stack Overflow questions using TF-IDF text representations and a Linear SVM classifier. The evaluation demonstrates that the model is capable of capturing meaningful relationships between technical text patterns and associated programming tags.\n",
        "\n",
        "### Key Insights\n",
        "- Multi-label recommendation problems require **ranking-based evaluation metrics** rather than traditional accuracy, as multiple tags may be correct for a single question.\n",
        "- **Feature engineering had a greater impact on performance than model selection**, highlighting the importance of informative text representations in NLP tasks.\n",
        "- Tag recommendation performance is influenced by **label competition**, where several related tags may be simultaneously relevant, limiting achievable Precision@K scores.\n",
        "- Decision-score ranking proved effective for selecting the most relevant predicted tags without requiring probability calibration.\n",
        "\n",
        "Overall, the results provide a strong baseline tag recommendation system and demonstrate the importance of structured data pipelines, ranking-aware evaluation, and careful feature engineering in real-world NLP applications.\n"
      ],
      "metadata": {
        "id": "JdlkXtlS9r-N"
      }
    }
  ]
}